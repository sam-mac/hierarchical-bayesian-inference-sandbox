{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14373f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.13\n",
    "# If needed in a fresh env:\n",
    "#   pip install \"pymc>=5.21\" \"arviz>=0.17\" \"numpy>=2\" \"pandas>=2.2\"\n",
    "from pytensor.tensor.variable import TensorVariable\n",
    "from pytensor.tensor import slinalg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az\n",
    "\n",
    "# ---- 1) Get synthetic data from helpers ----\n",
    "from bayes_tools.helpers.synthetic_data_helpers import (\n",
    "    make_hierarchical_ou_dataset,\n",
    ")\n",
    "\n",
    "# Build OU-level monthly panel\n",
    "df_ou = make_hierarchical_ou_dataset(\n",
    "    n_regions=1,\n",
    "    n_sites_per_region=3,\n",
    "    n_ous_per_site=3,\n",
    "    n_years=1,\n",
    "    wave_months=(6, 12),\n",
    "    wave_missing_prob=0.,\n",
    "    seed=7,\n",
    ")\n",
    "\n",
    "# (Optional) aggregate to parent level (e.g., 'site' or 'region')\n",
    "# df_site = aggregate_to_parent(df_ou, level=\"site\")\n",
    "\n",
    "# We'll model at the OU level:\n",
    "df = df_ou.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c729090",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 0) Data prep + indices (with coords)\n",
    "# -------------------------------\n",
    "df = df_ou.copy().sort_values([\"ou_code\",\"date\"]).reset_index(drop=True)\n",
    "\n",
    "# OU index\n",
    "ou_cat = df[\"ou_code\"].astype(\"category\")\n",
    "ou_idx = ou_cat.cat.codes.to_numpy().astype(\"int32\")\n",
    "ou_labels = ou_cat.cat.categories.astype(str).to_numpy()\n",
    "G = len(ou_labels)\n",
    "\n",
    "# Site mapping (fallback if not present)\n",
    "if \"site_code\" in df.columns:\n",
    "    ou_to_site = (df[[\"ou_code\",\"site_code\"]].drop_duplicates()\n",
    "                    .set_index(\"ou_code\").loc[ou_labels, \"site_code\"])\n",
    "    site_cat = ou_to_site.astype(\"category\")\n",
    "    site_labels = site_cat.cat.categories.astype(str).to_numpy()\n",
    "    site_of_ou = site_cat.cat.codes.to_numpy().astype(\"int32\")\n",
    "else:\n",
    "    site_labels = np.array([\"site0\"])\n",
    "    site_of_ou = np.zeros(G, dtype=\"int32\")\n",
    "S = len(site_labels)\n",
    "\n",
    "# Time index (monthly)\n",
    "dates = pd.to_datetime(df[\"date\"])\n",
    "unique_months = np.sort(dates.unique())\n",
    "T = len(unique_months)\n",
    "month_to_idx = {m: i for i, m in enumerate(unique_months)}\n",
    "time_idx = np.array([month_to_idx[d] for d in dates], dtype=\"int32\")\n",
    "\n",
    "# Observed outcomes/predictors\n",
    "y_raw = df[\"productivity\"].to_numpy(dtype=\"float64\")\n",
    "y = np.log(y_raw).astype(\"float64\")\n",
    "\n",
    "x_obs = df[\"survey_score\"].to_numpy(dtype=\"float64\")   # NaN where missing\n",
    "n_resp = df[\"n_respondents\"].to_numpy(dtype=\"float64\") # NaN where no survey\n",
    "\n",
    "n_resp_filled = np.where(np.isnan(n_resp), 0.0, n_resp).astype(\"float64\")\n",
    "has_survey = ~np.isnan(x_obs)\n",
    "idx_obs = np.flatnonzero(has_survey)\n",
    "\n",
    "# (Optional) Known survey schedule: e.g., months 6 and 12 of each year\n",
    "WAVE_MONTHS = (6, 12)\n",
    "is_scheduled = np.isin(pd.DatetimeIndex(dates).month.to_numpy(), WAVE_MONTHS)\n",
    "idx_sched = np.flatnonzero(is_scheduled)  # only these can be observed\n",
    "responded_sched = has_survey[idx_sched].astype(\"int8\")  # observed 1/0 on scheduled rows\n",
    "\n",
    "# Standardize y and x (using observed x only)\n",
    "def zscore(a: np.ndarray):\n",
    "    m = np.nanmean(a); s = np.nanstd(a); s = s if s > 0 else 1.0\n",
    "    return (a - m) / s, m, s\n",
    "\n",
    "y_z, y_mean, y_sd = zscore(y)\n",
    "x_z = x_obs.copy()\n",
    "if np.isfinite(x_obs[idx_obs]).any():\n",
    "    x_z, x_mean, x_sd = zscore(x_obs)\n",
    "else:\n",
    "    x_mean, x_sd = 0.0, 1.0\n",
    "    x_z = (x_obs - x_mean) / x_sd\n",
    "\n",
    "# Optional: data-informed prior scales for latent survey process (EB flavor)\n",
    "df_tmp = df.copy()\n",
    "df_tmp[\"x_z\"] = x_z\n",
    "between_sd_z = np.nanstd(df_tmp.groupby(\"ou_code\")[\"x_z\"].mean().to_numpy())\n",
    "within_sd_z = np.nanmedian(df_tmp.groupby(\"ou_code\")[\"x_z\"].std().to_numpy())\n",
    "between_sd_z = np.nan_to_num(between_sd_z, nan=1.0)\n",
    "within_sd_z = np.nan_to_num(within_sd_z, nan=1.0)\n",
    "\n",
    "# Shared arrays\n",
    "N = len(df)\n",
    "n_eff_np = np.clip(np.sqrt(np.maximum(n_resp_filled, 1.0)), 1.0, 1000.0)\n",
    "\n",
    "# -------------------------------\n",
    "# 1) Build model with coords/dims\n",
    "# -------------------------------\n",
    "coords = {\n",
    "    \"obs\": np.arange(N),\n",
    "    \"obs_svy\": np.arange(len(idx_obs)),\n",
    "    \"obs_sched\": np.arange(len(idx_sched)),\n",
    "    \"ou\": ou_labels,\n",
    "    \"site\": site_labels,\n",
    "    \"time\": pd.to_datetime(unique_months),\n",
    "    \"ab\": [\"alpha\",\"beta\"],  # just for labeling when needed\n",
    "}\n",
    "\n",
    "with pm.Model(coords=coords) as model:\n",
    "    # Index helpers (pm.Data so we can reuse)\n",
    "    ou_of_obs   = pm.Data(\"ou_of_obs\", ou_idx, dims=\"obs\")\n",
    "    time_of_obs = pm.Data(\"time_of_obs\", time_idx, dims=\"obs\")\n",
    "    site_of_ouD = pm.Data(\"site_of_ou\", site_of_ou, dims=\"ou\")\n",
    "    n_eff       = pm.Data(\"n_eff\", n_eff_np, dims=\"obs\")\n",
    "    obs_idx_svy = pm.Data(\"obs_idx_svy\", idx_obs, dims=\"obs_svy\")\n",
    "    x_z_obs     = pm.Data(\"x_z_obs\", x_z[idx_obs], dims=\"obs_svy\")\n",
    "    obs_idx_sched = pm.Data(\"obs_idx_sched\", idx_sched, dims=\"obs_sched\")\n",
    "    responded_schedD = pm.Data(\"responded_sched\", responded_sched, dims=\"obs_sched\")\n",
    "\n",
    "    # ------------------\n",
    "    # Global means\n",
    "    # ------------------\n",
    "    mu_alpha = pm.Normal(\"mu_alpha\", 0.0, 1.0)\n",
    "    mu_beta  = pm.Normal(\"mu_beta\",  0.0, 0.5)\n",
    "\n",
    "    # ------------------\n",
    "    # Site-level (alpha,beta) correlated random effects via LKJ\n",
    "    # ------------------\n",
    "    L_site, corr_site, sds_site = pm.LKJCholeskyCov(\n",
    "        \"L_site\", n=2, eta=2.0, sd_dist=pm.HalfNormal.dist(1.0),\n",
    "        compute_corr=True, store_in_trace=True\n",
    "    )\n",
    "    z_site = pm.Normal(\"z_site\", 0.0, 1.0, size=(S, 2))\n",
    "    assert isinstance(L_site, TensorVariable) and isinstance(corr_site, TensorVariable) and isinstance(sds_site, TensorVariable)\n",
    "    ab_site = z_site @ L_site.T     # (S,2)\n",
    "    alpha_site = pm.Deterministic(\"alpha_site\", ab_site[:, 0], dims=\"site\")\n",
    "    beta_site  = pm.Deterministic(\"beta_site\",  ab_site[:, 1], dims=\"site\")\n",
    "\n",
    "    # ------------------\n",
    "    # OU-level (alpha,beta) residual random effects via LKJ\n",
    "    # ------------------\n",
    "    L_ou, corr_ou, sds_ou = pm.LKJCholeskyCov(\n",
    "        \"L_ou\", n=2, eta=2.0, sd_dist=pm.HalfNormal.dist(1.0),\n",
    "        compute_corr=True, store_in_trace=True\n",
    "    )\n",
    "    z_ou = pm.Normal(\"z_ou\", 0.0, 1.0, size=(G, 2))\n",
    "    assert isinstance(L_ou, TensorVariable) and isinstance(corr_ou, TensorVariable) and isinstance(sds_ou, TensorVariable)\n",
    "    ab_ou = z_ou @ L_ou.T            # (G,2)\n",
    "    alpha_ou_off = ab_ou[:, 0]\n",
    "    beta_ou_off  = ab_ou[:, 1]\n",
    "\n",
    "    # Combine levels to get OU-specific coeffs\n",
    "    alpha_ou = pm.Deterministic(\n",
    "        \"alpha_ou\", mu_alpha + alpha_site[site_of_ouD] + alpha_ou_off, dims=\"ou\"\n",
    "    )\n",
    "    beta_ou = pm.Deterministic(\n",
    "        \"beta_ou\", mu_beta + beta_site[site_of_ouD] + beta_ou_off, dims=\"ou\"\n",
    "    )\n",
    "\n",
    "    # Helpful deterministics\n",
    "    pm.Deterministic(\"corr_site_alpha_beta\", corr_site[0, 1])\n",
    "    pm.Deterministic(\"corr_ou_alpha_beta\",   corr_ou[0, 1])\n",
    "\n",
    "    # ------------------\n",
    "    # Time effects with AR(1) structure (both outcome and survey processes)\n",
    "    # ------------------\n",
    "    def ar1_chol(phi: TensorVariable, sigma: TensorVariable, T: int):\n",
    "        t = pt.arange(T)\n",
    "        diff = pt.abs(t[:, None] - t[None, :])\n",
    "        R = pt.power(phi, diff)                      # correlation\n",
    "        variance = (sigma**2) / (1 - phi**2)         # stationary variance\n",
    "        Sigma = variance * R\n",
    "        Sigma = Sigma + 1e-6 * pt.eye(T)             # jitter\n",
    "        return slinalg.cholesky(Sigma)\n",
    "\n",
    "    # Outcome time effect λ_t\n",
    "    phi_lambda  = pm.Uniform(\"phi_lambda\", lower=-0.95, upper=0.95)\n",
    "    sigma_lambda = pm.HalfNormal(\"sigma_lambda\", 1.0)\n",
    "    lambda_time = pm.MvNormal(\n",
    "        \"lambda_time\",\n",
    "        mu=pt.zeros(T), chol=ar1_chol(phi_lambda, sigma_lambda, T),\n",
    "        dims=\"time\"\n",
    "    )\n",
    "    lambda_eff = pm.Deterministic(\"lambda_eff\", lambda_time - pt.mean(lambda_time), dims=\"time\")\n",
    "\n",
    "    # Survey time effect τ_t (for x*)\n",
    "    phi_tau  = pm.Uniform(\"phi_tau\", lower=-0.95, upper=0.95)\n",
    "    sigma_tau = pm.HalfNormal(\"sigma_tau\", 1.0)\n",
    "    tau_time = pm.MvNormal(\n",
    "        \"tau_time\",\n",
    "        mu=pt.zeros(T), chol=ar1_chol(phi_tau, sigma_tau, T),\n",
    "        dims=\"time\"\n",
    "    )\n",
    "    tau_eff = pm.Deterministic(\"tau_eff\", tau_time - pt.mean(tau_time), dims=\"time\")\n",
    "\n",
    "    # ------------------\n",
    "    # Latent survey process x* (z scale), with EB-scaled priors (optional)\n",
    "    # ------------------\n",
    "    mu_x = pm.Normal(\"mu_x\", 0.0, 1.0)\n",
    "    sigma_mu_x = pm.HalfNormal(\"sigma_mu_x\", max(0.3, 1.5 * between_sd_z))  # EB-flavored scale\n",
    "    mu_x_ou = pm.Normal(\"mu_x_ou\", mu_x, sigma_mu_x, dims=\"ou\")\n",
    "\n",
    "    gamma_time_x = pm.Normal(\"gamma_time_x\", 0.05, 0.3)  # learn time wiggle magnitude\n",
    "    sigma_x = pm.HalfNormal(\"sigma_x\", max(0.3, 1.5 * within_sd_z))\n",
    "\n",
    "    x_latent = pm.Normal(\n",
    "        \"x_latent\",\n",
    "        mu=mu_x_ou[ou_of_obs] + gamma_time_x * tau_eff[time_of_obs],\n",
    "        sigma=sigma_x,\n",
    "        dims=\"obs\",\n",
    "    )\n",
    "\n",
    "    # ------------------\n",
    "    # Survey measurement model (only at observed rows)\n",
    "    # ------------------\n",
    "    sigma_meas_base = pm.HalfNormal(\"sigma_meas_base\", 1.0)\n",
    "    sigma_meas = sigma_meas_base / n_eff\n",
    "    pm.Normal(\n",
    "        \"survey_obs\",\n",
    "        mu=x_latent[obs_idx_svy],\n",
    "        sigma=sigma_meas[obs_idx_svy],\n",
    "        observed=x_z_obs,\n",
    "    )\n",
    "\n",
    "    # ------------------\n",
    "    # Missingness mechanism (only on scheduled months)\n",
    "    #   logit P(survey present | scheduled) = ρ0 + ρx * x* + ρ_ou + ρ_time\n",
    "    # ------------------\n",
    "    rho0 = pm.Normal(\"rho0\", 0.0, 1.0)\n",
    "    rho_x = pm.Normal(\"rho_x\", 0.0, 1.0)\n",
    "    # OU random intercept for response\n",
    "    sigma_rho_ou = pm.HalfNormal(\"sigma_rho_ou\", 0.5)\n",
    "    rho_ou_raw = pm.Normal(\"rho_ou_raw\", 0.0, 1.0, dims=\"ou\")\n",
    "    rho_ou = pm.Deterministic(\"rho_ou\", sigma_rho_ou * rho_ou_raw, dims=\"ou\")\n",
    "    # Time random intercept (centered)\n",
    "    sigma_rho_t = pm.HalfNormal(\"sigma_rho_t\", 0.5)\n",
    "    rho_t_raw = pm.Normal(\"rho_t_raw\", 0.0, 1.0, dims=\"time\")\n",
    "    rho_t = pm.Deterministic(\"rho_t\", sigma_rho_t * (rho_t_raw - pt.mean(rho_t_raw)), dims=\"time\")\n",
    "\n",
    "    logit_p_sched = (\n",
    "        rho0\n",
    "        + rho_x * x_latent[obs_idx_sched]\n",
    "        + rho_ou[ou_of_obs[obs_idx_sched]]\n",
    "        + rho_t[time_of_obs[obs_idx_sched]]\n",
    "    )\n",
    "    pm.Bernoulli(\n",
    "        \"survey_present_given_scheduled\",\n",
    "        logit_p=logit_p_sched,\n",
    "        observed=responded_schedD,\n",
    "    )\n",
    "\n",
    "    # ------------------\n",
    "    # Outcome model: Student-t likelihood for robustness\n",
    "    # ------------------\n",
    "    sigma_y = pm.HalfNormal(\"sigma_y\", 0.5)\n",
    "    nu_y_raw = pm.Exponential(\"nu_y_raw\", 1/15)  # mean ~15\n",
    "    nu_y = pm.Deterministic(\"nu_y\", nu_y_raw + 2.0)  # ensure nu>2 (finite variance)\n",
    "\n",
    "    mu_y = (\n",
    "        alpha_ou[ou_of_obs]\n",
    "        + lambda_eff[time_of_obs]\n",
    "        + beta_ou[ou_of_obs] * x_latent\n",
    "    )\n",
    "    y_like = pm.StudentT(\"y_like\", nu=nu_y, mu=mu_y, sigma=sigma_y, observed=y_z)\n",
    "\n",
    "    # (optional global deterministics)\n",
    "    pm.Deterministic(\"beta_global\", mu_beta)\n",
    "    pm.Deterministic(\"alpha_global\", mu_alpha)\n",
    "\n",
    "    # Sampling (tune target_accept up if you see divergences)\n",
    "    idata = pm.sample(\n",
    "        draws=1500, tune=2000, chains=4, cores=4,\n",
    "        target_accept=0.95, random_seed=7, progressbar=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885b209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TWO WAYS TO MITIGATE OVERFITTING\n",
    "# # 1. Gaussian (ridge) shrinkage: beta ~ Normal(0, tau) with small/learned tau.\n",
    "# # 2. [SHOWN BELOW] (Regularized) Horseshoe for sparse-ish signals (many small, few large):\n",
    "# tau = pm.HalfStudentT(\"tau\", nu=3, sigma=0.5)      # global\n",
    "# lam = pm.HalfCauchy(\"lam\", beta=1, dims=\"cov\")     # local\n",
    "# c   = pm.InverseGamma(\"c\", 2, 2)                   # slab scale\n",
    "# slab = pt.sqrt((c**2 * lam**2) / (c**2 + tau**2 * lam**2))\n",
    "# beta = pm.Normal(\"beta\", 0, tau * lam * slab, dims=\"cov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e91afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_post_mean = idata.posterior[\"x_latent\"].mean((\"chain\",\"draw\")).values\n",
    "np.corrcoef(x_post_mean[idx_obs], x_z[idx_obs])[0,1]   # expect high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35527752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, arviz as az\n",
    "\n",
    "# Nice labels for OUs\n",
    "ou_labels = pd.Index(pd.Categorical(df[\"ou_code\"]).categories.astype(str), name=\"ou_code\")\n",
    "\n",
    "# Quick tables\n",
    "summ_alpha = az.summary(idata, var_names=[\"alpha_ou\"], hdi_prob=0.9)\n",
    "summ_beta  = az.summary(idata, var_names=[\"beta_ou\"],  hdi_prob=0.9)\n",
    "\n",
    "# Attach OU codes to summaries (indices like 'beta_ou[0]')\n",
    "def add_ou_labels(summ, ou_labels):\n",
    "    lab = []\n",
    "    for idx in summ.index:\n",
    "        i = int(idx.split(\"[\")[1].rstrip(\"]\"))\n",
    "        lab.append(ou_labels[i])\n",
    "    summ = summ.assign(ou_code=lab).set_index(\"ou_code\")\n",
    "    return summ\n",
    "\n",
    "summ_alpha = add_ou_labels(summ_alpha, ou_labels)\n",
    "summ_beta  = add_ou_labels(summ_beta,  ou_labels)\n",
    "\n",
    "print(\"Alpha per OU (posterior mean and 90% HDI):\")\n",
    "display(summ_alpha[[\"mean\",\"hdi_5%\",\"hdi_95%\"]])\n",
    "print(\"\\nBeta per OU (posterior mean and 90% HDI):\")\n",
    "display(summ_beta[[\"mean\",\"hdi_5%\",\"hdi_95%\"]])\n",
    "\n",
    "# Forest plots\n",
    "az.plot_forest(idata, var_names=[\"beta_ou\"], combined=True, hdi_prob=0.9, figsize=(8, 0.35*len(ou_labels)+2))\n",
    "plt.title(\"OU slopes β_g (x* → log-productivity, both z-scored)\")\n",
    "plt.show()\n",
    "\n",
    "az.plot_forest(idata, var_names=[\"alpha_ou\"], combined=True, hdi_prob=0.9, figsize=(8, 0.35*len(ou_labels)+2))\n",
    "plt.title(\"OU intercepts α_g (log-productivity z-score)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711e8cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, arviz as az\n",
    "# assumes: idata, y_sd, x_sd, df, ou_labels already defined\n",
    "\n",
    "# Pull posterior: dims ~ (chain, draw, beta_ou_dim_0)\n",
    "beta = idata.posterior[\"beta_ou\"]\n",
    "\n",
    "# Map to % change in raw productivity per +1 survey point\n",
    "mult = np.exp((y_sd / x_sd) * beta)\n",
    "pct  = (mult - 1.0) * 100.0  # dims: (chain, draw, ou)\n",
    "\n",
    "# Posterior mean & 90% interval over chain/draw\n",
    "pct_mean = pct.mean(dim=(\"chain\",\"draw\")).values  # shape: (G,)\n",
    "q = pct.quantile([0.05, 0.95], dim=(\"chain\",\"draw\"))          # dims: (quantile, ou)\n",
    "pct_lo = q.sel(quantile=0.05).values\n",
    "pct_hi = q.sel(quantile=0.95).values\n",
    "\n",
    "eff_df = pd.DataFrame({\n",
    "    \"ou_code\": pd.Index(pd.Categorical(df[\"ou_code\"]).categories.astype(str)),\n",
    "    \"pct_mean\": pct_mean, \"pct_lo\": pct_lo, \"pct_hi\": pct_hi\n",
    "}).set_index(\"ou_code\").sort_values(\"pct_mean\")\n",
    "\n",
    "display(eff_df)\n",
    "\n",
    "# Plot interval per OU\n",
    "fig, ax = plt.subplots(figsize=(8, 0.35*len(eff_df)+2))\n",
    "ypos = np.arange(len(eff_df))\n",
    "ax.hlines(y=ypos, xmin=eff_df[\"pct_lo\"], xmax=eff_df[\"pct_hi\"])\n",
    "ax.plot(eff_df[\"pct_mean\"], ypos, \"o\")\n",
    "ax.axvline(0, ls=\"--\", lw=1)\n",
    "ax.set_yticks(ypos, eff_df.index)\n",
    "ax.set_xlabel(\"% change in raw productivity per +1 survey point (90% interval)\")\n",
    "ax.set_title(\"OU-specific effect interpretation\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c0e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick tables\n",
    "summ_alpha = az.summary(idata, var_names=[\"alpha_ou\"], hdi_prob=0.9)\n",
    "summ_beta  = az.summary(idata, var_names=[\"beta_ou\"],  hdi_prob=0.9)\n",
    "\n",
    "def add_ou_labels(summ, labels):\n",
    "    labs = []\n",
    "    for idx in summ.index:\n",
    "        i = int(idx.split(\"[\")[1].rstrip(\"]\"))\n",
    "        labs.append(labels[i])\n",
    "    return summ.assign(ou_code=labs).set_index(\"ou_code\")\n",
    "\n",
    "ou_labels = pd.Index(pd.Categorical(df[\"ou_code\"]).categories.astype(str), name=\"ou_code\")\n",
    "display(add_ou_labels(summ_alpha, ou_labels)[[\"mean\",\"hdi_5%\",\"hdi_95%\"]])\n",
    "display(add_ou_labels(summ_beta,  ou_labels)[[\"mean\",\"hdi_5%\",\"hdi_95%\"]])\n",
    "\n",
    "# Forest plots\n",
    "az.plot_forest(idata, var_names=[\"beta_ou\"], combined=True, hdi_prob=0.9,\n",
    "               figsize=(8, 0.35*len(ou_labels)+2))\n",
    "plt.title(\"OU slopes β_g (x* → log-productivity, both z-scored)\"); plt.show()\n",
    "\n",
    "az.plot_forest(idata, var_names=[\"alpha_ou\"], combined=True, hdi_prob=0.9,\n",
    "               figsize=(8, 0.35*len(ou_labels)+2))\n",
    "plt.title(\"OU intercepts α_g (log-productivity z-score)\"); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6997aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "def plot_latent_for_ou(ou_code_str):\n",
    "    labels = pd.Categorical(df[\"ou_code\"]).categories.astype(str)\n",
    "    g = int(np.where(labels == ou_code_str)[0][0])\n",
    "\n",
    "    # rows for this OU, ordered in time\n",
    "    row_idx_all = np.where(pd.Categorical(df[\"ou_code\"]).codes == g)[0]\n",
    "    t = time_idx[row_idx_all]\n",
    "    order = np.argsort(t)\n",
    "    row_idx = row_idx_all[order]\n",
    "    months = pd.to_datetime(unique_months[t[order]])\n",
    "\n",
    "    # posterior summaries for x_latent at those rows\n",
    "    x_da  = idata.posterior[\"x_latent\"]                     # (chain, draw, obs)\n",
    "    x_mu  = x_da.mean((\"chain\",\"draw\")).values[row_idx]\n",
    "    qx    = x_da.quantile([0.05, 0.95], dim=(\"chain\",\"draw\"))\n",
    "    lo    = qx.sel(quantile=0.05).values[row_idx]\n",
    "    hi    = qx.sel(quantile=0.95).values[row_idx]\n",
    "\n",
    "    # observed standardized survey at those rows\n",
    "    obs_mask = has_survey[row_idx]\n",
    "    obs_months = months[obs_mask]\n",
    "    obs_vals   = x_z[row_idx][obs_mask]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "    ax.fill_between(months, lo, hi, alpha=0.25, label=\"x* 90% interval\")\n",
    "    ax.plot(months, x_mu, lw=2, label=\"x* posterior mean\")\n",
    "    ax.scatter(obs_months, obs_vals, marker=\"x\", s=60, label=\"observed survey (z)\")\n",
    "    ax.set_title(f\"Latent survey x* over time — OU {ou_code_str}\")\n",
    "    ax.set_ylabel(\"Standardized survey (z)\")\n",
    "    ax.set_xlabel(\"Month\")\n",
    "    ax.legend()\n",
    "    ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(ax.xaxis.get_major_locator()))\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# Example\n",
    "plot_latent_for_ou(str(ou_labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1459b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick plot: OU intercept vs slope post means\n",
    "a_mu = idata.posterior[\"alpha_ou\"].mean((\"chain\",\"draw\")).to_numpy()\n",
    "b_mu = idata.posterior[\"beta_ou\"].mean((\"chain\",\"draw\")).to_numpy()\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(a_mu, b_mu)\n",
    "for i, lab in enumerate(ou_labels):\n",
    "    plt.annotate(str(lab), (a_mu[i], b_mu[i]), fontsize=8, xytext=(3,3), textcoords=\"offset points\")\n",
    "plt.axhline(0, lw=1, ls=\"--\"); plt.axvline(0, lw=1, ls=\"--\")\n",
    "plt.xlabel(\"α_g (mean log-productivity z)\"); plt.ylabel(\"β_g (slope)\")\n",
    "plt.title(\"OU intercepts vs slopes (posterior means)\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Some single-parameter posteriors\n",
    "az.plot_trace(idata, var_names=[\"beta_global\",\"corr_alpha_beta\",\"sigma_y\",\"sigma_x\",\"sigma_meas_base\"], compact=True, figsize=(10,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f69155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"PATH\"] = \"/opt/homebrew/bin:\" + os.environ[\"PATH\"]\n",
    "# # Optional: be explicit\n",
    "# os.environ[\"GRAPHVIZ_DOT\"] = \"/opt/homebrew/bin/dot\"\n",
    "# pm.model_to_graphviz(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738da813",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = pm.model_to_graphviz(model)\n",
    "print(g.source[:1000])           # preview DOT text\n",
    "g.save(\"model.dot\")              # write DOT to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5182afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os \n",
    "print (\"dot:\", shutil.which(\"dot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0feee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "os.environ[\"PATH\"] = \"/opt/homebrew/bin:\" + os.environ[\"PATH\"]\n",
    "print(shutil.which(\"dot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc6caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = pm.model_to_graphviz(model)\n",
    "g.graph_attr.update(rankdir=\"LR\")  # optional layout\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7631da29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"PATH\"] = \"/opt/homebrew/bin:\" + os.environ[\"PATH\"]\n",
    "# # Optional: be explicit\n",
    "# os.environ[\"GRAPHVIZ_DOT\"] = \"/opt/homebrew/bin/dot\"\n",
    "# pm.model_to_graphviz(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738da813",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = pm.model_to_graphviz(model)\n",
    "print(g.source[:1000])           # preview DOT text\n",
    "g.save(\"model.dot\")              # write DOT to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5182afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os \n",
    "print (\"dot:\", shutil.which(\"dot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7631da29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- 5) Quick checks ----\n",
    "az.summary(idata, var_names=[\n",
    "    \"mu_alpha\", \"mu_beta\", \"sigma_y\", \"sigma_x\", \"sigma_mu_x\",\n",
    "    \"sigma_meas_base\", \"corr_alpha_beta\"\n",
    "], kind=\"stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39040e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea26943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hierarchical-bayesian-inference-sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
